üß† American Sign Language (ASL) Alphabet Recognition using Deep Learning
This project demonstrates an ASL alphabet recognition system built using Convolutional Neural Networks (CNNs). Designed to bridge the communication gap between the deaf or hard-of-hearing community and non-signers, the model accurately interprets ASL gestures and converts them into text in real time.

We trained the model on a dataset of 87,000 images, applying one-hot encoding, RGB normalization, and data splitting strategies. The architecture includes multiple Conv2D, BatchNormalization, MaxPooling, Dropout, and Dense layers to ensure high performance and generalization. The final model achieved a training accuracy of 99.46% and testing accuracy of 91.09%, showing strong real-world potential.

![image](https://github.com/user-attachments/assets/5f3a3e2c-1b69-4f3d-b5dc-d2403eeb741c)


üîç Key Features:

CNN-based real-time ASL alphabet recognition

Preprocessing with normalization and one-hot encoding

High-accuracy classification across 29 ASL signs (A-Z + special characters)

Potential for use in healthcare, gesture interfaces, and accessibility tools

üöÄ Future Enhancements:

Data augmentation

Attention mechanisms

Multimodal input (e.g., depth/video)

Real-time UI development

Results:

![image](https://github.com/user-attachments/assets/5a819a57-0ee5-4b44-8506-e114b0c1c2db)

